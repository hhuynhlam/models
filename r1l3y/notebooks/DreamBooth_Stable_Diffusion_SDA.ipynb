{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJBCX5lfvgtV"
   },
   "source": [
    "# 1-click execution of Dreambooth Stable Diffusion\n",
    "- July 19,2023 - Updated accelerate version, again.\n",
    "- Jun 8, 2023 - Updated accelerate version.\n",
    "- May 24, 2023 - Fixed pytorch version error, again.\n",
    "\n",
    "\n",
    "\n",
    "Tutorials and prompts at [stable-diffusion-art.com](https://stable-diffusion-art.com)\n",
    "\n",
    "The latest copy can be found [here](https://colab.research.google.com/github/sagiodev/stablediffusion_webui/blob/master/DreamBooth_Stable_Diffusion_SDA.ipynb).\n",
    "\n",
    "Modified from [Shivam Shrirao](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth)'s repo.\n",
    "\n",
    "Instructions - See [acommpanying article](https://stable-diffusion-art.com/dreambooth/) for step-by-step walkthrough. Leave comment under article if you have any questions.\n",
    "1. Make sure your Google Drive has at least 2GB (4GB if `fp16` is not checked)\n",
    "2. Prepare you custom image to be 512x512 pixels.\n",
    "1. Run the first cell by clicking the play button.\n",
    "2. Grant access to Google Drive.\n",
    "3. Click \"Choose files\" to upload your images\n",
    "4. After it is complete (800 steps should take 30 mins), go to your [Google Drive](https://drive.google.com/drive/my-drive) to download your model in folder Dreambooth_model\n",
    "5. Use the 2nd cell to generate new images using the new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XU7NuMAA2drw",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "709e24d2-6978-4678-dd17-03fe460cabcd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time_start = time.time()\n",
    "#@title DreamBooth\n",
    "HUGGINGFACE_TOKEN = \"\"\n",
    "\n",
    "#@markdown Name/Path of the initial model. (Find model name [here](https://huggingface.co/models))\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
    "BRANCH = \"fp16\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter instance prompt and class prompt.\\\n",
    "#@markdown Example 1: photo of zwx person, photo of a person\\\n",
    "#@markdown Example 2: photo of zwx toy, photo of a toy\n",
    "instance_prompt = \"photo of zwx toy\" #@param {type:\"string\"}\n",
    "class_prompt =  \"photo of a toy\" #@param {type:\"string\"}\n",
    "training_steps = 800 #@param {type:\"integer\"}\n",
    "learning_rate = 1e-6 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown  Convert to fp16? (takes half the space (2GB)).\n",
    "fp16 = True #@param {type: \"boolean\"}\n",
    "#@markdown  Compile xformers (Try only if you see xformers error. Will take 1 more hour).\n",
    "complie_xformers = False #@param {type: \"boolean\"}\n",
    "\n",
    "save_to_gdrive = True\n",
    "from google.colab import drive\n",
    "if save_to_gdrive:\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "#@markdown Clear log after run?\n",
    "CLEAR_LOG = False #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"stable_diffusion_weights/output\"\n",
    "OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
    "\n",
    "# Check type of GPU and VRAM available.\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "\n",
    "\n",
    "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
    "\n",
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      instance_prompt,\n",
    "        \"class_prompt\":         class_prompt,\n",
    "        \"instance_data_dir\":    \"/content/data/instance\",\n",
    "        \"class_data_dir\":       \"/content/data/class\"\n",
    "    },\n",
    "#     {\n",
    "#         \"instance_prompt\":      \"photo of ukj person\",\n",
    "#         \"class_prompt\":         \"photo of a person\",\n",
    "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
    "#         \"class_data_dir\":       \"/content/data/person\"\n",
    "#     }\n",
    "]\n",
    "\n",
    "# `class_data_dir` contains regularization images\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)\n",
    "\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "for c in concepts_list:\n",
    "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
    "        shutil.move(filename, dst_path)\n",
    "\n",
    "\n",
    "def clear():\n",
    "    from IPython.display import clear_output; return clear_output()\n",
    "\n",
    "\n",
    "\n",
    "# huggingface token\n",
    "!mkdir -p ~/.huggingface\n",
    "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
    "\n",
    "\n",
    "# install repos\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
    "# install xformer wheel\n",
    "print('Install xformers')\n",
    "if complie_xformers:\n",
    "  %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg=xformers\n",
    "#else:\n",
    "#  %pip install  --no-deps -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
    "#%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/4c06c79_various6/xformers-0.0.15.dev0_4c06c79.d20221201-cp38-cp38-linux_x86_64.whl\n",
    "#%pip install -q https://github.com/ShivamShrirao/xformers-wheels/releases/download/4c06c79/xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "\n",
    "############## Edit this section to customize parameters\n",
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --revision=$BRANCH \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=$training_steps \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"$instance_prompt\" \\\n",
    "  --concepts_list=\"concepts_list.json\"\n",
    "\n",
    "########################################\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory).\n",
    "\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "weightdirs = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))\n",
    "if len(weightdirs) == 0:\n",
    "  raise KeyboardInterrupt(\"No training weights directory found\")\n",
    "WEIGHTS_DIR = weightdirs[-1]\n",
    "\n",
    "\n",
    "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")\n",
    "\n",
    "\n",
    "if CLEAR_LOG:\n",
    "  clear()\n",
    "\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")\n",
    "minutes = (time.time()-time_start)/60\n",
    "print(\"Dreambooth completed successfully. It took %1.1f minutes.\"%minutes)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap='gray')\n",
    "        currAxes.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grid.png', dpi=72)\n",
    "\n",
    "if save_to_gdrive:\n",
    "  import os.path\n",
    "  gPath = \"/content/drive/MyDrive/Dreambooth_model\"\n",
    "  !mkdir -p $gPath\n",
    "  filename = 'model.ckpt'\n",
    "  i = 1\n",
    "  ckpt_gpath = gPath + '/' + filename\n",
    "  while os.path.isfile(ckpt_gpath):\n",
    "    filename = 'model%d.ckpt'%i\n",
    "    ckpt_gpath = gPath + '/' + filename\n",
    "    i += 1\n",
    "  ckpt_gpath = gPath + '/' + filename\n",
    "  !cp $ckpt_path $ckpt_gpath\n",
    "  print('Model saved to %s'%ckpt_gpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install deps for next cell\n",
    "!{sys.executable} -m pip install torch diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1d449dcf46c64d138404771ea0dda3cd",
      "fdaccd9b53cd4f9599861588ab74064c",
      "002d22452d4d4126b31ac4fc1e7e705b",
      "b7d586a3fdd846cfb78d173400ef83dc",
      "a017ece116a7499bbc0b869e7a4fa698",
      "43b68e1d9e764a2ab212405790fc7a11",
      "f83391bbbfcf429a9fba859fd493b075",
      "4bcdec1408d04101a387816574ea19fd",
      "882ea6763fd64f64a1428198dbe88d92",
      "6c173afdbfe14956ac231bb66b83abf1",
      "4b828c6bddde4e07ab7f8b6b5c59298c"
     ]
    },
    "id": "K6xoHWSsbcS3",
    "outputId": "5c590d67-b4cb-4d7b-cc28-da785d0825a3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m#@param {type:\"number\"}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m#@param {type:\"number\"}\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline, DDIMScheduler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#@title Test image generation from model\n",
    "\n",
    "prompt = \"oil painting of R1l3y dog in style of van gogh\" #@param {type:\"string\"}\n",
    "negative_prompt = \"\" #@param {type:\"string\"}\n",
    "num_samples = 2 #@param {type:\"number\"}\n",
    "guidance_scale = 7.5 #@param {type:\"number\"}\n",
    "num_inference_steps = 30 #@param {type:\"number\"}\n",
    "height = 512 #@param {type:\"number\"}\n",
    "width = 512 #@param {type:\"number\"}\n",
    "seed = 100 #@param {type:\"number\"}\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "save_to_gdrive = True\n",
    "from google.colab import drive\n",
    "if save_to_gdrive:\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "model_path = \"/content/drive/Dreambooth_model/model.ckpt\"             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
    "if 'pipe' not in locals():\n",
    "  scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "  g_cuda = None\n",
    "\n",
    "\n",
    "\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "g_cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "                prompt, height=int(height), width=int(width),\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_images_per_prompt=int(num_samples),\n",
    "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                generator=g_cuda\n",
    "            ).images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lJoOgLQHnC8L",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
    "\n",
    "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
    "import shutil\n",
    "from glob import glob\n",
    "import os\n",
    "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
    "    if f != WEIGHTS_DIR:\n",
    "        shutil.rmtree(f)\n",
    "        print(\"Deleted\", f)\n",
    "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
    "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
    "        try:\n",
    "            shutil.rmtree(f)\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "        print(\"Deleted\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX",
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Start Training\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002d22452d4d4126b31ac4fc1e7e705b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bcdec1408d04101a387816574ea19fd",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_882ea6763fd64f64a1428198dbe88d92",
      "value": 30
     }
    },
    "1d449dcf46c64d138404771ea0dda3cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdaccd9b53cd4f9599861588ab74064c",
       "IPY_MODEL_002d22452d4d4126b31ac4fc1e7e705b",
       "IPY_MODEL_b7d586a3fdd846cfb78d173400ef83dc"
      ],
      "layout": "IPY_MODEL_a017ece116a7499bbc0b869e7a4fa698"
     }
    },
    "43b68e1d9e764a2ab212405790fc7a11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b828c6bddde4e07ab7f8b6b5c59298c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bcdec1408d04101a387816574ea19fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c173afdbfe14956ac231bb66b83abf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "882ea6763fd64f64a1428198dbe88d92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a017ece116a7499bbc0b869e7a4fa698": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7d586a3fdd846cfb78d173400ef83dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c173afdbfe14956ac231bb66b83abf1",
      "placeholder": "​",
      "style": "IPY_MODEL_4b828c6bddde4e07ab7f8b6b5c59298c",
      "value": " 30/30 [00:07&lt;00:00, 10.02it/s]"
     }
    },
    "f83391bbbfcf429a9fba859fd493b075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdaccd9b53cd4f9599861588ab74064c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43b68e1d9e764a2ab212405790fc7a11",
      "placeholder": "​",
      "style": "IPY_MODEL_f83391bbbfcf429a9fba859fd493b075",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
